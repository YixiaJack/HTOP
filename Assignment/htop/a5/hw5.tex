% --- LaTeX Homework Template - S. Venkatraman ---

% --- Set document class and font size ---

\documentclass[letterpaper, 11pt]{article}

% --- Package imports ---

\usepackage{
  amsmath, amsthm, amssymb, mathtools, dsfont,	  % Math typesetting
  graphicx, wrapfig, subfig, float,                  % Figures and graphics formatting
  listings, color, inconsolata, pythonhighlight,     % Code formatting
  fancyhdr, sectsty, hyperref, enumerate, enumitem } % Headers/footers, section fonts, links, lists

% --- Page layout settings ---

% Set page margins
\usepackage[left=1.35in, right=1.35in, bottom=1in, top=1.1in, headsep=0.2in]{geometry}

% Anchor footnotes to the bottom of the page
\usepackage[bottom]{footmisc}

% Set line spacing
\renewcommand{\baselinestretch}{1.2}

% Set spacing between paragraphs
\setlength{\parskip}{1.5mm}

% Allow multi-line equations to break onto the next page
\allowdisplaybreaks

% Enumerated lists: make numbers flush left, with parentheses around them
\setlist[enumerate]{wide=0pt, leftmargin=21pt, labelwidth=0pt, align=left}
\setenumerate[1]{label={(\arabic*)}}

% --- Page formatting settings ---

% Set link colors for labeled items (blue) and citations (red)
\hypersetup{colorlinks=true, linkcolor=blue, citecolor=red}

% Make reference section title font smaller
\renewcommand{\refname}{\large\bf{References}}

% --- Settings for printing computer code ---

% Define colors for green text (comments), grey text (line numbers),
% and green frame around code
\definecolor{greenText}{rgb}{0.5, 0.7, 0.5}
\definecolor{greyText}{rgb}{0.5, 0.5, 0.5}
\definecolor{codeFrame}{rgb}{0.5, 0.7, 0.5}

% Define code settings
\lstdefinestyle{code} {
  frame=single, rulecolor=\color{codeFrame},            % Include a green frame around the code
  numbers=left,                                         % Include line numbers
  numbersep=8pt,                                        % Add space between line numbers and frame
  numberstyle=\tiny\color{greyText},                    % Line number font size (tiny) and color (grey)
  commentstyle=\color{greenText},                       % Put comments in green text
  basicstyle=\linespread{1.1}\ttfamily\footnotesize,    % Set code line spacing
  keywordstyle=\ttfamily\footnotesize,                  % No special formatting for keywords
  showstringspaces=false,                               % No marks for spaces
  xleftmargin=1.95em,                                   % Align code frame with main text
  framexleftmargin=1.6em,                               % Extend frame left margin to include line numbers
  breaklines=true,                                      % Wrap long lines of code
  postbreak=\mbox{\textcolor{greenText}{$\hookrightarrow$}\space} % Mark wrapped lines with an arrow
}

% Set all code listings to be styled with the above settings
\lstset{style=code}

% --- Math/Statistics commands ---

% Add a reference number to a single line of a multi-line equation
% Usage: "\numberthis\label{labelNameHere}" in an align or gather environment
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

% Shortcut for bold text in math mode, e.g. $\b{X}$
\let\b\mathbf

% Shortcut for bold Greek letters, e.g. $\bg{\beta}$
\let\bg\boldsymbol

% Shortcut for calligraphic script, e.g. %\mc{M}$
\let\mc\mathcal

% \mathscr{(letter here)} is sometimes used to denote vector spaces
\usepackage[mathscr]{euscript}

% Convergence: right arrow with optional text on top
% E.g. $\converge[w]$ for weak convergence
\newcommand{\converge}[1][]{\xrightarrow{#1}}

% Normal distribution: arguments are the mean and variance
% E.g. $\normal{\mu}{\sigma}$
\newcommand{\normal}[2]{\mathcal{N}\left(#1,#2\right)}

% Uniform distribution: arguments are the left and right endpoints
% E.g. $\unif{0}{1}$
\newcommand{\unif}[2]{\text{Uniform}(#1,#2)}

% Independent and identically distributed random variables
% E.g. $ X_1,...,X_n \iid \normal{0}{1}$
\newcommand{\iid}{\stackrel{\smash{\text{iid}}}{\sim}}

% Equality: equals sign with optional text on top
% E.g. $X \equals[d] Y$ for equality in distribution
\newcommand{\equals}[1][]{\stackrel{\smash{#1}}{=}}

% Math mode symbols for common sets and spaces. Example usage: $\R$
\newcommand{\R}{\mathbb{R}}   % Real numbers
\newcommand{\C}{\mathbb{C}}   % Complex numbers
\newcommand{\Q}{\mathbb{Q}}   % Rational numbers
\newcommand{\Z}{\mathbb{Z}}   % Integers
\newcommand{\N}{\mathbb{N}}   % Natural numbers
\newcommand{\F}{\mathcal{F}}  % Calligraphic F for a sigma algebra
\newcommand{\El}{\mathcal{L}} % Calligraphic L, e.g. for L^p spaces

% Math mode symbols for probability
\newcommand{\pr}{\mathbb{P}}    % Probability measure
\newcommand{\E}{\mathbb{E}}     % Expectation, e.g. $\E(X)$
\newcommand{\var}{\text{Var}}   % Variance, e.g. $\var(X)$
\newcommand{\cov}{\text{Cov}}   % Covariance, e.g. $\cov(X,Y)$
\newcommand{\corr}{\text{Corr}} % Correlation, e.g. $\corr(X,Y)$
\newcommand{\B}{\mathcal{B}}    % Borel sigma-algebra

% Other miscellaneous symbols
\newcommand{\tth}{\text{th}}	% Non-italicized 'th', e.g. $n^\tth$
\newcommand{\Oh}{\mathcal{O}}	% Big-O notation, e.g. $\O(n)$
\newcommand{\1}{\mathds{1}}	% Indicator function, e.g. $\1_A$

% Additional commands for math mode
\DeclareMathOperator*{\argmax}{argmax}    % Argmax, e.g. $\argmax_{x\in[0,1]} f(x)$
\DeclareMathOperator*{\argmin}{argmin}    % Argmin, e.g. $\argmin_{x\in[0,1]} f(x)$
\DeclareMathOperator*{\spann}{Span}       % Span, e.g. $\spann\{X_1,...,X_n\}$
\DeclareMathOperator*{\bias}{Bias}        % Bias, e.g. $\bias(\hat\theta)$
\DeclareMathOperator*{\ran}{ran}          % Range of an operator, e.g. $\ran(T) 
\DeclareMathOperator*{\dv}{d\!}           % Non-italicized 'with respect to', e.g. $\int f(x) \dv x$
\DeclareMathOperator*{\diag}{diag}        % Diagonal of a matrix, e.g. $\diag(M)$
\DeclareMathOperator*{\trace}{trace}      % Trace of a matrix, e.g. $\trace(M)$

% Numbered theorem, lemma, etc. settings - e.g., a definition, lemma, and theorem appearing in that 
% order in Section 2 will be numbered Definition 2.1, Lemma 2.2, Theorem 2.3. 
% Example usage: \begin{theorem}[Name of theorem] Theorem statement \end{theorem}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}

% Un-numbered theorem, lemma, etc. settings
% Example usage: \begin{lemma*}[Name of lemma] Lemma statement \end{lemma*}
\newtheorem*{theorem*}{Theorem}
\newtheorem*{proposition*}{Proposition}
\newtheorem*{lemma*}{Lemma}
\newtheorem*{corollary*}{Corollary}
\newtheorem*{definition*}{Definition}
\newtheorem*{example*}{Example}
\newtheorem*{remark*}{Remark}
\newtheorem*{claim}{Claim}

% --- Left/right header text (to appear on every page) ---

% Include a line underneath the header, no footer line
\pagestyle{fancy}
\renewcommand{\footrulewidth}{0pt}
\renewcommand{\headrulewidth}{0.4pt}

% Left header text: course name/assignment number
\lhead{MATH-SHU 238 (HTOP) -- Homework 5}

% Right header text: your name
\rhead{Yixia Yu (yy5091@nyu.edu)}

% --- Document starts here ---

\begin{document}
\subsection*{Problem 1}
Random social networks. Let G = (V,E) be a random graph with m = |V| vertices and edge-set E. Write $d_v$ for the degree of vertex v, that is, the number of edges meeting at v. Let Y be a uniformly chosen vertex, and Z a uniformly chosen neighbour of Y.
\\(a) Show that $Ed_Z \geq Ed_Y$.
\\(b) Interpret this inequality when the vertices represent people, and the edges represent friendship.
\begin{proof}
    \subsection*{(a) Show that $Ed_Z \geq Ed_Y$.}

First, we compute $E[d_Y]$. Since $Y$ is uniformly chosen from $V$:
\begin{align*}
E[d_Y] &= \sum_{v \in V} d_v \cdot P(Y = v) = \sum_{v \in V} d_v \cdot \frac{1}{m} = \frac{1}{m} \sum_{v \in V} d_v
\end{align*}

Since $\sum_{v \in V} d_v = 2|E|$, we have $E[d_Y] = \frac{2|E|}{m}$.

For $E[d_Z]$, we need to analyze how $Z$ is selected. For a vertex $u$:
\begin{align*}
P(Z = u) &= \sum_{v \in V} P(Z = u | Y = v) \cdot P(Y = v) \\
&= \frac{1}{m} \sum_{v: (u,v) \in E} \frac{1}{d_v}
\end{align*}

Therefore:
\begin{align*}
E[d_Z] &= \sum_{u \in V} d_u \cdot P(Z = u) \\
&= \frac{1}{m} \sum_{u \in V} d_u \sum_{v \in N(u)} \frac{1}{d_v} \\
&= \frac{1}{m} \sum_{(u,v) \in E} \frac{d_u}{d_v}
\end{align*}

By symmetry, this equals:
\begin{align*}
E[d_Z] &= \frac{1}{2m} \sum_{(u,v) \in E} \left(\frac{d_u}{d_v} + \frac{d_v}{d_u}\right)
\end{align*}

Since $\frac{d_u}{d_v} + \frac{d_v}{d_u} \geq 2$ by the AM-GM inequality:
\begin{align*}
E[d_Z] &\geq \frac{1}{2m} \sum_{(u,v) \in E} 2 = \frac{2|E|}{2m} = \frac{2|E|}{m} = E[d_Y]
\end{align*}

Thus, $E[d_Z] \geq E[d_Y]$.

\subsection*{(b) Interpret this inequality when the vertices represent people, and the edges represent friendship.}

This inequality represents the "Friendship Paradox" which states that, on average, your friends have more friends than you do. This occurs because individuals with many connections are overrepresented in friendship networks. When people compare their social circles to those of their friends, they often feel less connected, creating a perception bias. This mathematical property helps explain social phenomena like feelings of inadequacy on social media, and has practical applications in monitoring trends and information diffusion in populations.
\end{proof}
\subsection*{Problem 2}
In your pocket is a random number $N$ of coins, where $N$ has the Poisson distribution with parameter $\lambda.$ You toss each coin once, with heads showing with probability $p$ each time. Show that the total number of heads has the Poisson distribution with parameter $\lambda p.$
\begin{proof}
    
Let $N \sim \text{Poisson}(\lambda)$ represent the random number of coins in your pocket. Each coin is tossed once, with probability $p$ of showing heads. Let $H$ denote the total number of heads observed. We aim to show that $H \sim \text{Poisson}(\lambda p)$.

Using the law of total probability, we can write:
\begin{align*}
P(H = x) &= \sum_{n=x}^{\infty} P(H = x | N = n) \cdot P(N = n)
\end{align*}

Since $H|N=n \sim \text{Binomial}(n,p)$ and $N \sim \text{Poisson}(\lambda)$, we have:
\begin{align*}
P(H = x) &= \sum_{n=x}^{\infty} \binom{n}{x} p^x (1-p)^{n-x} \cdot \frac{\lambda^n e^{-\lambda}}{n!} \\
&= \sum_{n=x}^{\infty} \frac{n!}{x!(n-x)!} p^x (1-p)^{n-x} \cdot \frac{\lambda^n e^{-\lambda}}{n!} \\
&= \frac{p^x e^{-\lambda}}{x!} \sum_{n=x}^{\infty} \frac{\lambda^n(1-p)^{n-x}}{(n-x)!}
\end{align*}

Let's extract common factors:
\begin{align*}
P(H = x) &= \frac{p^x \lambda^x e^{-\lambda}}{x!} \sum_{n=x}^{\infty} \frac{\lambda^{n-x}(1-p)^{n-x}}{(n-x)!} \\
&= \frac{(p\lambda)^x e^{-\lambda}}{x!} \sum_{n=x}^{\infty} \frac{[\lambda(1-p)]^{n-x}}{(n-x)!}
\end{align*}

Making a change of variables with $m = n-x$, the sum becomes:
\begin{align*}
P(H = x) &= \frac{(p\lambda)^x e^{-\lambda}}{x!} \sum_{m=0}^{\infty} \frac{[\lambda(1-p)]^{m}}{m!} \\
&= \frac{(p\lambda)^x e^{-\lambda}}{x!} \cdot e^{\lambda(1-p)}
\end{align*}

Since $e^{\lambda(1-p)} = e^{\lambda-\lambda p}$, we get:
\begin{align*}
P(H = x) &= \frac{(p\lambda)^x e^{-\lambda}}{x!} \cdot e^{\lambda-\lambda p} \\
&= \frac{(p\lambda)^x e^{-\lambda p}}{x!}
\end{align*}

This is precisely the probability mass function of a Poisson distribution with parameter $\lambda p$, completing our proof that $H \sim \text{Poisson}(\lambda p)$.
\end{proof}
\subsection*{Problem 3}
Compound Poisson distribution. Let Λ be a positive random variable with density function f and distribution function F, and let Y have the Poisson distribution with parameter Λ. Show for n = 0, 1, 2, ... that

$$\mathbb{P}(Y \leq n) = \int_{0}^{\infty} p_n(\lambda)F(\lambda)\,d\lambda,\quad\quad\mathbb{P}(Y > n) = \int_{0}^{\infty} p_n(\lambda)[1 - F(\lambda)]\,d\lambda,$$

where $p_n(\lambda) = e^{-\lambda}\lambda^n/n!$.
\begin{proof}
    First, let's calculate $\mathbb{P}(Y \leq n)$ using conditional probability and the law of total probability. Given that $\Lambda = \lambda$, $Y$ follows a Poisson distribution with parameter $\lambda$, so:

\begin{align*}
\mathbb{P}(Y \leq n) &= \int_{0}^{\infty} \mathbb{P}(Y \leq n | \Lambda = \lambda) f(\lambda) \, d\lambda \\
&= \int_{0}^{\infty} \sum_{r=0}^{n} \frac{e^{-\lambda}\lambda^r}{r!} f(\lambda) \, d\lambda \\
&= \sum_{r=0}^{n} \int_{0}^{\infty} \frac{e^{-\lambda}\lambda^r}{r!} f(\lambda) \, d\lambda \\
&= \sum_{r=0}^{n} \int_{0}^{\infty} p_r(\lambda) f(\lambda) \, d\lambda
\end{align*}

Now, we'll apply integration by parts to each term in the sum. For each $r$, let:
\begin{align*}
u = F(\lambda) \quad \text{and} \quad dv = p_r(\lambda) \, d\lambda
\end{align*}

Then $du = f(\lambda) \, d\lambda$ and $v$ needs to be determined by integrating $p_r(\lambda)$. We also know that:
\begin{align*}
\frac{d}{d\lambda}p_r(\lambda) &= \frac{d}{d\lambda}\left(\frac{e^{-\lambda}\lambda^r}{r!}\right) \\
&= \frac{1}{r!}\left(-e^{-\lambda}\lambda^r + e^{-\lambda}r\lambda^{r-1}\right) \\
&= -p_r(\lambda) + \frac{r}{\lambda}p_r(\lambda)
\end{align*}

For $r < n$, we can establish a recurrence relation:
\begin{align*}
p_{r+1}(\lambda) = \frac{\lambda}{r+1}p_r(\lambda)
\end{align*}

Using integration by parts:
\begin{align*}
\int_{0}^{\infty} p_r(\lambda) f(\lambda) \, d\lambda &= \left[ p_r(\lambda) F(\lambda) \right]_{0}^{\infty} - \int_{0}^{\infty} F(\lambda) \frac{d}{d\lambda}p_r(\lambda) \, d\lambda \\
&= \left[ p_r(\lambda) F(\lambda) \right]_{0}^{\infty} - \int_{0}^{\infty} F(\lambda) \left(-p_r(\lambda) + \frac{r}{\lambda}p_r(\lambda)\right) \, d\lambda \\
&= \left[ p_r(\lambda) F(\lambda) \right]_{0}^{\infty} + \int_{0}^{\infty} F(\lambda)p_r(\lambda) \, d\lambda - \int_{0}^{\infty} F(\lambda)\frac{r}{\lambda}p_r(\lambda) \, d\lambda \\
\end{align*}

The boundary term $\left[ p_r(\lambda) F(\lambda) \right]_{0}^{\infty}$ evaluates to $0$ because:
- At $\lambda = 0$, $p_r(0) = 0$ for $r > 0$ and $F(0) = 0$ for a positive random variable
- As $\lambda \to \infty$, $p_r(\lambda) \to 0$ exponentially while $F(\lambda) \to 1$

Using the recurrence relation and simplifying:
\begin{align*}
\int_{0}^{\infty} p_r(\lambda) f(\lambda) \, d\lambda &= \int_{0}^{\infty} F(\lambda)p_r(\lambda) \, d\lambda - \int_{0}^{\infty} F(\lambda)\frac{r+1}{\lambda}p_{r+1}(\lambda) \, d\lambda \\
\end{align*}

Summing over all $r$ from $0$ to $n$:
\begin{align*}
\sum_{r=0}^{n} \int_{0}^{\infty} p_r(\lambda) f(\lambda) \, d\lambda &= \sum_{r=0}^{n} \int_{0}^{\infty} F(\lambda)p_r(\lambda) \, d\lambda - \sum_{r=0}^{n-1} \int_{0}^{\infty} F(\lambda)p_{r+1}(\lambda) \, d\lambda \\
&= \int_{0}^{\infty} F(\lambda)p_n(\lambda) \, d\lambda
\end{align*}

The telescoping sum cancels out all terms except the last one with $p_n(\lambda)$. Therefore:
\begin{align*}
\mathbb{P}(Y \leq n) = \int_{0}^{\infty} p_n(\lambda)F(\lambda) \, d\lambda
\end{align*}

For the second part, we have:
\begin{align*}
\mathbb{P}(Y > n) &= 1 - \mathbb{P}(Y \leq n) \\
&= 1 - \int_{0}^{\infty} p_n(\lambda)F(\lambda) \, d\lambda
\end{align*}

Since $\int_{0}^{\infty} p_n(\lambda) \, d\lambda = 1$ (as $p_n(\lambda)$ can be viewed as a gamma distribution density with appropriate normalization), we have:
\begin{align*}
\mathbb{P}(Y > n) &= \int_{0}^{\infty} p_n(\lambda) \, d\lambda - \int_{0}^{\infty} p_n(\lambda)F(\lambda) \, d\lambda \\
&= \int_{0}^{\infty} p_n(\lambda)(1-F(\lambda)) \, d\lambda
\end{align*}
\end{proof}
\subsection*{Problem 4}
Mutual information. Let X and Y be discrete random variables with joint mass function f.
(a) Show that E(log $f_{X}(X)$) ≥ E(log $f_{Y}(X)$).
(b) Show that the mutual information

$$I = \mathbb{E}\left(\log \left\{\frac{f(X, Y)}{f_{X}(X)f_{Y}(Y)}\right\}\right)$$

satisfies I ≥ 0, with equality if and only if X and Y are independent.
\begin{proof}
    \textbf{(a)} We'll use the inequality $\log y \leq y - 1$ with equality if and only if $y = 1$.

Setting $y = \frac{f_Y(X)}{f_X(X)}$ and taking expectations:
\begin{align*}
\mathbb{E}\left[\log\left(\frac{f_Y(X)}{f_X(X)}\right)\right] &\leq \mathbb{E}\left[\frac{f_Y(X)}{f_X(X)} - 1\right]\\
&= \sum_x f_X(x)\left(\frac{f_Y(x)}{f_X(x)} - 1\right)\\
&= \sum_x f_Y(x) - \sum_x f_X(x)\\
&= 1 - 1 = 0
\end{align*}

Therefore:
\begin{align*}
\mathbb{E}[\log f_X(X)] &\geq \mathbb{E}[\log f_Y(X)]
\end{align*}

Equality holds if and only if $f_Y = f_X$.

\textbf{(b)} For the mutual information:
\begin{align*}
I = \mathbb{E}\left[\log\left(\frac{f(X,Y)}{f_X(X)f_Y(Y)}\right)\right]
\end{align*}

Applying the same inequality with $y = \frac{f_X(X)f_Y(Y)}{f(X,Y)}$:
\begin{align*}
-\log\left(\frac{f(X,Y)}{f_X(X)f_Y(Y)}\right) &\leq \frac{f_X(X)f_Y(Y)}{f(X,Y)} - 1
\end{align*}

Taking the expectation and multiplying by $-1$:
\begin{align*}
I &\geq \mathbb{E}\left[1 - \frac{f_X(X)f_Y(Y)}{f(X,Y)}\right]\\
&= 1 - \sum_{x,y}f_X(x)f_Y(y) = 0
\end{align*}

Therefore, $I \geq 0$, with equality if and only if $f(x,y) = f_X(x)f_Y(y)$ for all $(x,y)$, which is equivalent to $X$ and $Y$ being independent.
\end{proof}
\subsection*{Problem 5}
Let X be a non-negative random variable with density function f. Show that
$$\mathbb{E}(X^r) = \int_0^\infty r x^{r-1} \mathbb{P}(X > x) dx$$
for any $r \geq 1$ for which the expectation is finite.
\begin{proof}
We'll use integration by parts. First, by definition:
$$\mathbb{E}(X^r) = \int_0^\infty x^r f(x) dx$$

Now, let's work with the right side of the equation to be proven:
$$\int_0^\infty r x^{r-1} \mathbb{P}(X > x) dx$$

Note that $\mathbb{P}(X > x) = \int_x^\infty f(t) dt$ and set:
\begin{align*}
u &= \mathbb{P}(X > x) = \int_x^\infty f(t) dt\\
dv &= r x^{r-1} dx
\end{align*}

Then:
\begin{align*}
du &= -f(x) dx\\
v &= x^r
\end{align*}

Using the integration by parts formula $\int u dv = uv - \int v du$:
\begin{align*}
\int_0^\infty r x^{r-1} \mathbb{P}(X > x) dx &= \left[ x^r \mathbb{P}(X > x) \right]_0^\infty - \int_0^\infty x^r (-f(x)) dx\\
&= \lim_{a \to \infty} a^r \mathbb{P}(X > a) - 0 \cdot \mathbb{P}(X > 0) + \int_0^\infty x^r f(x) dx
\end{align*}

To complete the proof, we need to show that $\lim_{a \to \infty} a^r \mathbb{P}(X > a) = 0$.

Since $\mathbb{E}(X^r)$ is finite, we can apply Markov's inequality:
$$\mathbb{P}(X > a) \leq \frac{\mathbb{E}(X^r)}{a^r}$$

Therefore:
$$a^r \mathbb{P}(X > a) \leq \mathbb{E}(X^r)$$

As $a \to \infty$, we must have $a^r \mathbb{P}(X > a) \to 0$, otherwise $\mathbb{E}(X^r)$ would not be finite.

Thus:
\begin{align*}
\int_0^\infty r x^{r-1} \mathbb{P}(X > x) dx &= 0 + \int_0^\infty x^r f(x) dx\\
&= \mathbb{E}(X^r)
\end{align*}

This completes the proof that $\mathbb{E}(X^r) = \int_0^\infty r x^{r-1} \mathbb{P}(X > x) dx$ for any $r \geq 1$ where $\mathbb{E}(X^r) < \infty$.
\end{proof}
\subsection*{Problem 6}
Log-normal distribution. Let $Y=e^{X}$ where $X$ has the $N(0,1)$ distribution. Find the density function of $Y$.
\begin{proof}
    Let $Y = e^X$ where $X \sim N(0,1)$. To find the density function of $Y$, we use the change of variable technique.

The CDF of $Y$ is:
\begin{align*}
F_Y(y) = P(Y \leq y) = P(e^X \leq y) = P(X \leq \ln y) = \Phi(\ln y)
\end{align*}

Differentiating to get the PDF:
\begin{align*}
f_Y(y) &= \frac{d}{dy}F_Y(y) = \frac{d}{dy}\Phi(\ln y)\\
&= \phi(\ln y) \cdot \frac{1}{y}
\end{align*}

Substituting $\phi(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$:
\begin{align*}
f_Y(y) &= \frac{1}{\sqrt{2\pi}}e^{-\frac{(\ln y)^2}{2}} \cdot \frac{1}{y}\\
&= \frac{1}{\sqrt{2\pi} \cdot y}e^{-\frac{(\ln y)^2}{2}}, \quad y > 0
\end{align*}

This is the standard log-normal distribution.
\end{proof}
\subsection*{Problem 7}
Show that $E|X| < \infty$ if and only if the following holds: for all $\epsilon > 0$, there exists $\delta > 0$, such that $E(|X|I_A) < \epsilon$ for all $A$ such that $P(A) < \delta$.
\begin{proof}
    We need to show that $E|X| < \infty$ if and only if for all $\epsilon > 0$, there exists $\delta > 0$ such that $E(|X|I_A) < \epsilon$ for all $A$ with $P(A) < \delta$.

\textbf{(Forward Direction)} Assume $E|X| < \infty$.

Since $E|X| < \infty$, by monotone convergence, $E(|X|I_{\{|X|>z\}}) \rightarrow 0$ as $z \rightarrow \infty$.

For any $\epsilon > 0$, choose $z$ large enough so that $E(|X|I_{\{|X|>z\}}) < \frac{\epsilon}{2}$.

For any event $A$, we can write:
\begin{align*}
E(|X|I_A) &= E(|X|I_A I_{\{|X| \leq z\}}) + E(|X|I_A I_{\{|X| > z\}})\\
&\leq z \cdot P(A) + E(|X|I_{\{|X| > z\}})\\
&< z \cdot P(A) + \frac{\epsilon}{2}
\end{align*}

Now set $\delta = \frac{\epsilon}{2z}$. Then for any $A$ with $P(A) < \delta$:
\begin{align*}
E(|X|I_A) &< z \cdot \frac{\epsilon}{2z} + \frac{\epsilon}{2} = \epsilon
\end{align*}

\textbf{(Reverse Direction)} Assume the condition holds.

For a fixed $\epsilon > 0$, there exists $\delta > 0$ such that $E(|X|I_A) < \epsilon$ whenever $P(A) < \delta$.

By Markov's inequality, $P(|X| > t) \leq \frac{E|X|}{t}$, so $P(|X| > t) \rightarrow 0$ as $t \rightarrow \infty$ (regardless of whether $E|X|$ is finite).

Choose $t_0$ large enough that $P(|X| > t_0) < \delta$. Let $A = \{|X| > t_0\}$.

Then $E(|X|I_A) = E(|X|I_{\{|X| > t_0\}}) < \epsilon$.

Now we can show that $\int_{-y}^y |u| dF_X(u)$ forms a Cauchy sequence as $y \rightarrow \infty$:

For $y > t_0$:
\begin{align*}
\int_{-y}^y |u| dF_X(u) &\leq \int_{-t_0}^{t_0} |u| dF_X(u) + \int_{\{|u|>t_0\} \cap \{|u|\leq y\}} |u| dF_X(u)\\
&\leq \int_{-t_0}^{t_0} |u| dF_X(u) + E(|X|I_{\{|X| > t_0\}})\\
&< \int_{-t_0}^{t_0} |u| dF_X(u) + \epsilon
\end{align*}

This shows that $\int_{-y}^y |u| dF_X(u)$ converges as $y \rightarrow \infty$, implying that $E|X| < \infty$.

Therefore, $E|X| < \infty$ if and only if the given condition holds.
\end{proof}
\subsection*{Problem 8}
Let $(\Omega, \mathcal{F}, \mu)$ be a measure space, $f$ be a non-negative simple function. Show that $\nu(A) := \int_A f d\mu$ defines a new measure.
\begin{proof}
    Since $f$ can be written as $f = \sum_{i=1}^n a_i I_{E_i}$ where $a_i \geq 0$ are constants and $E_i \in \mathcal{F}$ are measurable sets.

For any $A \in \mathcal{F}$, we have:
\begin{align*}
\nu(A) &= \int_A f d\mu\\
&= \int_A \sum_{i=1}^n a_i I_{E_i} d\mu\\
&= \sum_{i=1}^n a_i \int_A I_{E_i} d\mu\\
&= \sum_{i=1}^n a_i \mu(A \cap E_i)
\end{align*}

To prove $\nu$ is a measure, we need to verify three properties:

\textbf{1. Non-negativity:} For any $A \in \mathcal{F}$, $\nu(A) \geq 0$.

This is clear since $f \geq 0$, so $\int_A f d\mu \geq 0$ for any measurable set $A$.

\textbf{2. Empty set:} $\nu(\emptyset) = 0$.

We have:
\begin{align*}
\nu(\emptyset) &= \int_{\emptyset} f d\mu = 0
\end{align*}
since the integral over a set of measure zero is zero.

\textbf{3. Countable additivity:} If $\{A_j\}_{j=1}^{\infty}$ is a sequence of disjoint measurable sets, then $\nu\left(\bigcup_{j=1}^{\infty} A_j\right) = \sum_{j=1}^{\infty} \nu(A_j)$.

Let $\{A_j\}_{j=1}^{\infty}$ be a sequence of disjoint sets in $\mathcal{F}$. We have:

\begin{align*}
\nu\left(\bigcup_{j=1}^{\infty} A_j\right) &= \int_{\bigcup_{j=1}^{\infty} A_j} f d\mu\\
&= \int_{\bigcup_{j=1}^{\infty} A_j} \sum_{i=1}^n a_i I_{E_i} d\mu\\
&= \sum_{i=1}^n a_i \int_{\bigcup_{j=1}^{\infty} A_j} I_{E_i} d\mu\\
&= \sum_{i=1}^n a_i \mu\left(E_i \cap \bigcup_{j=1}^{\infty} A_j\right)\\
\end{align*}

Since $\mu$ is a measure and $E_i \cap A_j$ are disjoint for different $j$, we have:
\begin{align*}
\mu\left(E_i \cap \bigcup_{j=1}^{\infty} A_j\right) &= \mu\left(\bigcup_{j=1}^{\infty} (E_i \cap A_j)\right)\\
&= \sum_{j=1}^{\infty} \mu(E_i \cap A_j)
\end{align*}

Therefore:
\begin{align*}
\nu\left(\bigcup_{j=1}^{\infty} A_j\right) &= \sum_{i=1}^n a_i \sum_{j=1}^{\infty} \mu(E_i \cap A_j)\\
&= \sum_{j=1}^{\infty} \sum_{i=1}^n a_i \mu(E_i \cap A_j)\\
&= \sum_{j=1}^{\infty} \int_{A_j} \sum_{i=1}^n a_i I_{E_i} d\mu\\
&= \sum_{j=1}^{\infty} \int_{A_j} f d\mu\\
&= \sum_{j=1}^{\infty} \nu(A_j)
\end{align*}

We have verified all three properties, so $\nu(A) = \int_A f d\mu$ defines a measure on $(\Omega, \mathcal{F})$.
\end{proof}
\end{document}
